"use strict";
// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.SparkPool = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * Manages a Synapse Spark Pool.
 *
 * ## Import
 *
 * Synapse Spark Pool can be imported using the `resource id`, e.g.
 *
 * ```sh
 *  $ pulumi import azure:synapse/sparkPool:SparkPool example /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/group1/providers/Microsoft.Synapse/workspaces/workspace1/bigDataPools/sparkPool1
 * ```
 */
class SparkPool extends pulumi.CustomResource {
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["autoPause"] = state ? state.autoPause : undefined;
            resourceInputs["autoScale"] = state ? state.autoScale : undefined;
            resourceInputs["cacheSize"] = state ? state.cacheSize : undefined;
            resourceInputs["computeIsolationEnabled"] = state ? state.computeIsolationEnabled : undefined;
            resourceInputs["dynamicExecutorAllocationEnabled"] = state ? state.dynamicExecutorAllocationEnabled : undefined;
            resourceInputs["libraryRequirement"] = state ? state.libraryRequirement : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["nodeCount"] = state ? state.nodeCount : undefined;
            resourceInputs["nodeSize"] = state ? state.nodeSize : undefined;
            resourceInputs["nodeSizeFamily"] = state ? state.nodeSizeFamily : undefined;
            resourceInputs["sessionLevelPackagesEnabled"] = state ? state.sessionLevelPackagesEnabled : undefined;
            resourceInputs["sparkConfig"] = state ? state.sparkConfig : undefined;
            resourceInputs["sparkEventsFolder"] = state ? state.sparkEventsFolder : undefined;
            resourceInputs["sparkLogFolder"] = state ? state.sparkLogFolder : undefined;
            resourceInputs["sparkVersion"] = state ? state.sparkVersion : undefined;
            resourceInputs["synapseWorkspaceId"] = state ? state.synapseWorkspaceId : undefined;
            resourceInputs["tags"] = state ? state.tags : undefined;
        }
        else {
            const args = argsOrState;
            if ((!args || args.nodeSize === undefined) && !opts.urn) {
                throw new Error("Missing required property 'nodeSize'");
            }
            if ((!args || args.nodeSizeFamily === undefined) && !opts.urn) {
                throw new Error("Missing required property 'nodeSizeFamily'");
            }
            if ((!args || args.synapseWorkspaceId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'synapseWorkspaceId'");
            }
            resourceInputs["autoPause"] = args ? args.autoPause : undefined;
            resourceInputs["autoScale"] = args ? args.autoScale : undefined;
            resourceInputs["cacheSize"] = args ? args.cacheSize : undefined;
            resourceInputs["computeIsolationEnabled"] = args ? args.computeIsolationEnabled : undefined;
            resourceInputs["dynamicExecutorAllocationEnabled"] = args ? args.dynamicExecutorAllocationEnabled : undefined;
            resourceInputs["libraryRequirement"] = args ? args.libraryRequirement : undefined;
            resourceInputs["name"] = args ? args.name : undefined;
            resourceInputs["nodeCount"] = args ? args.nodeCount : undefined;
            resourceInputs["nodeSize"] = args ? args.nodeSize : undefined;
            resourceInputs["nodeSizeFamily"] = args ? args.nodeSizeFamily : undefined;
            resourceInputs["sessionLevelPackagesEnabled"] = args ? args.sessionLevelPackagesEnabled : undefined;
            resourceInputs["sparkConfig"] = args ? args.sparkConfig : undefined;
            resourceInputs["sparkEventsFolder"] = args ? args.sparkEventsFolder : undefined;
            resourceInputs["sparkLogFolder"] = args ? args.sparkLogFolder : undefined;
            resourceInputs["sparkVersion"] = args ? args.sparkVersion : undefined;
            resourceInputs["synapseWorkspaceId"] = args ? args.synapseWorkspaceId : undefined;
            resourceInputs["tags"] = args ? args.tags : undefined;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(SparkPool.__pulumiType, name, resourceInputs, opts);
    }
    /**
     * Get an existing SparkPool resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new SparkPool(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of SparkPool.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === SparkPool.__pulumiType;
    }
}
exports.SparkPool = SparkPool;
/** @internal */
SparkPool.__pulumiType = 'azure:synapse/sparkPool:SparkPool';
//# sourceMappingURL=sparkPool.js.map